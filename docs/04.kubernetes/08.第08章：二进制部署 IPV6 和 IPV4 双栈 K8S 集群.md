---
title: 第08章：二进制部署 IPV6 和 IPV4 双栈 K8S 集群
date: 2022-07-09 09:31:43
permalink: /pages/953157/
categories:
  - kubernetes
tags:
  - 
author: 
  name: 小阳爱技术
  link: https://blog.abck8s.com
---
# 第08章：二进制部署 IPV6 和 IPV4 双栈 K8S 集群

二进制方式部署是企业中常用的一种部署方式，因为它具有一定的灵活性和可拓展性。

## 一、部署之前的一些优化

在部署之前，我们需要先优化系统，下载不要软件。

### 1.1. 部署系统版本

| 软件            | 版本                                                         |
| --------------- | ------------------------------------------------------------ |
| CentOS          | CentOS Linux release 7.9.2009 (Core)                         |
| Containerd      | 1.6.6-3.1                                                    |
| Kubernetes      | V1.24-2                                                      |
| Calico          | V3.23                                                        |
| Kernel-lt       | [kernel-lt-5.4.203-1.el7.elrepo.x86_64.rpm](https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-lt-5.4.203-1.el7.elrepo.x86_64.rpm) |
| Kernel-lt-devel | [kernel-lt-devel-5.4.203-1.el7.elrepo.x86_64.rpm](https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-lt-devel-5.4.203-1.el7.elrepo.x86_64.rpm) |

### 1.2. 节点规划

| Hostname  | Ipv4          | Ipv6                    | 内核版本  |
| --------- | ------------- | ----------------------- | --------- |
| k8s-m-001 | 192.168.15.81 | fd15:4ba5:5a2b:1008::81 | 5.4.203-1 |
| K8s-m-002 | 192.168.15.82 | fd15:4ba5:5a2b:1008::82 | 5.4.203-1 |
| K8s-m-003 | 192.168.15.83 | fd15:4ba5:5a2b:1008::83 | 5.4.203-1 |
| K8s-n-001 | 192.168.15.84 | fd15:4ba5:5a2b:1008::84 | 5.4.203-1 |
| K8s-n-002 | 192.168.15.85 | fd15:4ba5:5a2b:1008::85 | 5.4.203-1 |
| lb01      | 192.168.15.91 | fd15:4ba5:5a2b:1008::91 | 5.4.203-1 |
| lb02      | 192.168.15.92 | fd15:4ba5:5a2b:1008::92 | 5.4.203-1 |
| vip       | 192.168.15.93 |                         | 5.4.203-1 |

### 1.3. 关闭selinux

```bash
# 永久关闭
sed -i 's#enforcing#disabled#g' /etc/selinux/config
# 零时关闭
setenforce 0
```

### 1.4.配置 Hosts

```bash
fd15:4ba5:5a2b:1008::81 k8s-m-001
fd15:4ba5:5a2b:1008::82 k8s-m-002
fd15:4ba5:5a2b:1008::83 k8s-m-003
fd15:4ba5:5a2b:1008::84 k8s-n-001
fd15:4ba5:5a2b:1008::85 k8s-n-002

192.168.15.81 k8s-m-001
192.168.15.82 k8s-m-002
192.168.15.83 k8s-m-003
192.168.15.84 k8s-n-001
192.168.15.85 k8s-n-002

192.168.15.91 lb01
192.168.15.92 lb02
192.168.15.93 lb-vip
```

### 1.5. 配置国内yum源

默认情况下，CentOS使用的是官方yum源，所以一般情况下在国内使用是非常慢，所以我们可以替换成国内的一些比较成熟的yum源，例如：清华大学镜像源，网易云镜像源等等。

```bash
$ mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup

$ curl -o /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo

$ curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo

# 刷新缓存
$ yum makecache

# 更新系统
$ yum update -y --exclud=kernel*
```

### 1.6. 升级内核版本

由于Docker运行需要较新的系统内核功能，例如ipvs等等，所以一般情况下，我们需要使用4.0+以上版本的系统内核。

```bash
# 内核要求是4.18+，如果是`CentOS 8`则不需要升级内核

wget https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-lt-5.4.203-1.el7.elrepo.x86_64.rpm
wget https://elrepo.org/linux/kernel/el7/x86_64/RPMS/kernel-lt-devel-5.4.204-1.el7.elrepo.x86_64.rpm

yum localinstall -y kernel-lt*

grub2-set-default  0 && grub2-mkconfig -o /etc/grub2.cfg

grubby --default-kernel

# 重启
reboot
```

### 1.7. 安装ipvs

ipvs是系统内核中的一个模块，其网络转发性能很高。一般情况下，我们首选ipvs。

```bash
# 安装IPVS
yum install -y conntrack-tools ipvsadm ipset conntrack libseccomp

# 加载IPVS模块
cat > /etc/sysconfig/modules/ipvs.modules <<EOF
#!/bin/bash
ipvs_modules="ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack"
for kernel_module in \${ipvs_modules}; do
  /sbin/modinfo -F filename \${kernel_module} > /dev/null 2>&1
  if [ $? -eq 0 ]; then
    /sbin/modprobe \${kernel_module}
  fi
done
EOF

chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep ip_vs
```

### 1.8. 内核参数优化

内核参数优化的主要目的是使其更适合kubernetes的正常运行。

```bash
cat > /etc/sysctl.d/k8s.conf << EOF
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-iptables = 1
net.bridge.bridge-nf-call-ip6tables = 1
fs.may_detach_mounts = 1
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
fs.file-max=52706963
fs.nr_open=52706963

net.ipv4.tcp_keepalive_time = 600
net.ipv4.tcp.keepaliv.probes = 3
net.ipv4.tcp_keepalive_intvl = 15
net.ipv4.tcp.max_tw_buckets = 36000
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp.max_orphans = 327680
net.ipv4.tcp_orphan_retries = 3
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.ip_conntrack_max = 65536
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.top_timestamps = 0
net.core.somaxconn = 16384

net.ipv6.conf.all.disable_ipv6 = 0
net.ipv6.conf.default.disable_ipv6 = 0
net.ipv6.conf.lo.disable_ipv6 = 0
net.ipv6.conf.all.forwarding = 1

EOF

# 立即生效
sysctl --system

```

### 1.9. 安装基础软件

安装一些基础软件，是为了方便我们的日常使用。

```bash
yum install wget expect vim net-tools ntp bash-completion ipvsadm ipset jq iptables conntrack sysstat libseccomp -y
```

### 1.10. 关闭防火墙

关闭防火墙是为了方便日常使用，不会给我们造成困扰。在生成环境中建议打开。

```bash
systemctl disable --now firewalld
```
### 1.11. 同步集群时间

在集群当中，时间是一个很重要的概念，一旦集群当中某台机器时间跟集群时间不一致，可能会导致集群面临很多问题。所以，在部署集群之前，需要同步集群当中的所有机器的时间。

```bash
yum install ntp -y

ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
echo 'Asia/Shanghai' > /etc/timezone

ntpdate time2.aliyun.com

# 写入定时任务
echo "0 */1 * * * /usr/sbin/ntpdate ntp1.aliyun.com &>/dev/null" >> /var/spool/cron/root
```

## 二、安装Containerd

containerd 作为底层运行时，必须在安装 k8s 之前部署好。

```bash
wget https://github.com/containerd/containerd/releases/download/v1.6.6/cri-containerd-cni-1.6.6-linux-amd64.tar.gz

tar -xf  cri-containerd-cni-1.6.6-linux-amd64.tar.gz -C /

#创建服务启动文件
cat > /etc/systemd/system/containerd.service <<EOF
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target local-fs.target

[Service]
ExecStartPre=-/sbin/modprobe overlay
ExecStart=/usr/local/bin/containerd
Type=notify
Delegate=yes
KillMode=process
Restart=always
RestartSec=5
LimitNPROC=infinity
LimitCORE=infinity
LimitNOFILE=infinity
TasksMax=infinity
OOMScoreAdjust=-999

[Install]
WantedBy=multi-user.target
EOF
```

### 2.1 配置Containerd所需的模块

```bash
cat <<EOF | sudo tee /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF
```

### 2.2 加载模块

```bash
systemctl restart systemd-modules-load.service
```

### 2.3 配置Containerd所需的内核

```bash
cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF
 
# 加载内核
 
sysctl --system
```

### 2.4 创建Containerd的配置文件

```bash
mkdir -p /etc/containerd
containerd config default | tee /etc/containerd/config.toml
 
 
修改Containerd的配置文件
sed -i "s#SystemdCgroup\ \=\ false#SystemdCgroup\ \=\ true#g" /etc/containerd/config.toml
 
cat /etc/containerd/config.toml | grep SystemdCgroup
 
sed -i "s#k8s.gcr.io#registry.aliyuncs.com/google_containers#g" /etc/containerd/config.toml
 
cat /etc/containerd/config.toml | grep sandbox_image
```

### 2.5 安装crictl

```bash
wget https://github.com/kubernetes-sigs/cri-tools/releases/download/v1.24.2/crictl-v1.24.2-linux-amd64.tar.gz

tar -xf crictl-v1.24.2-linux-amd64.tar.gz -C /usr/bin/

# 可执行可不执行
cat > /etc/crictl.yaml <<EOF
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
EOF
```

### 2.6 安装 CNI 插件

```bash
wget https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz

mkdir -p /etc/cni/net.d /opt/cni/bin 

tar -xf cni-plugins-linux-amd64-v1.1.1.tgz -C /opt/cni/bin/

systemctl daemon-reload
systemctl enable --now containerd
```



## 三、创建 k8s 集群证书

在 k8s 集群中基本上所有的组件都需要通过证书加密，所以在创建 k8s 集群之前需要创建证书。

### 3.1 安装 cfssl 证书生成工具

本次我们使用cfssl证书生成工具，这是一款把预先的证书机构、使用期等时间写在json文件里面会更加高效和自动化。cfssl采用go语言编写，是一个开源的证书管理工具，cfssljson用来从cfssl程序获取json输出，并将证书，密钥，csr和bundle写入文件中。

```bash
# 下载
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64

# 设置执行权限
chmod +x cfssljson_linux-amd64
chmod +x cfssl_linux-amd64

# 移动到/usr/local/bin
mv cfssljson_linux-amd64 cfssljson
mv cfssl_linux-amd64 cfssl
mv cfssljson cfssl /usr/local/bin
```

### 3.2 创建集群根证书

从整个架构来看，集群环境中最重要的部分就是etcd和API server。所以集群当中的证书都是针对etcd和api server来设置的。

所谓根证书，是CA认证中心与用户建立信任关系的基础，用户的数字证书必须有一个受信任的根证书，用户的数字证书才是有效的。从技术上讲，证书其实包含三部分，用户的信息，用户的公钥，以及证书签名。CA负责数字证书的批审、发放、归档、撤销等功能，CA颁发的数字证书拥有CA的数字签名，所以除了CA自身，其他机构无法不被察觉的改动。

```bash
mkdir -p /opt/cert/ca
cd /opt/cert/ca
cat > /opt/cert/ca/ca-config.json <<EOF
{
  "signing": {
    "default": {
      "expiry": "8760h"
    },
    "profiles": {
      "kubernetes": {
        "usages": [
          "signing",
          "key encipherment",
          "server auth",
          "client auth"
        ],
           "expiry": "8760h"
      }
    }
  }
}
EOF
```

- default是默认策略，指定证书默认有效期是1年
- profiles是定义使用场景，这里只是kubernetes，其实可以定义多个场景，分别指定不同的过期时间,使用场景等参数,后续签名证书时使用某个profile;
- signing: 表示该证书可用于签名其它证书,生成的ca.pem证书
- server auth: 表示client 可以用该CA 对server 提供的证书进行校验;
- client auth: 表示server 可以用该CA 对client 提供的证书进行验证。

### 3.3 创建根CA证书签名请求文件

```bash
cat > /opt/cert/ca/ca-csr.json << EOF
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names":[{
    "C": "CN",
    "ST": "ShangHai",
    "L": "ShangHai"
  }]
}
EOF
```

### 3.4 生成根证书

```bash
cfssl gencert -initca ca-csr.json | cfssljson -bare ca -
```

| 参数项   | 解释                          |
| -------- | ----------------------------- |
| gencert  | 生成新的key（密钥）和签名证书 |
| --initca | 初始化一个新CA证书            |

## 四、安装 Etcd 集群

Etcd是基于Raft的分布式key-value存储系统，由CoreOS团队开发，常用于服务发现，共享配置，以及并发控制（如leader选举，分布式锁等等）。Kubernetes使用Etcd进行状态和数据存储!

### 4.1 ETCD集群规划

在企业中部署 Etcd 集群最好使用单独的服务器。

| ETCD节点 | IP          |
| -------- | ----------- |
| Etcd-01  | 172.16.0.81 |
| Etcd-02  | 172.16.0.82 |
| Etcd-03  | 172.16.0.83 |

### 4.2 创建ETCD证书

```bash
mkdir -p /opt/cert/etcd
cd /opt/cert/etcd

cat > etcd-csr.json << EOF
{
    "CN": "etcd",
    "hosts": [
    "127.0.0.1",
    "192.168.15.81",
    "192.168.15.82",
    "192.168.15.83",
    "192.168.15.84",
    "192.168.15.85",
    "192.168.15.93"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
          "C": "CN",
          "ST": "ShangHai",
          "L": "ShangHai"
        }
    ]
}
EOF
```

### 4.3 生成 Etcd 证书

```bash
cfssl gencert -ca=../ca/ca.pem -ca-key=../ca/ca-key.pem -config=../ca/ca-config.json -profile=kubernetes etcd-csr.json | cfssljson -bare etcd
```

| 参数项   | 解释                                                         |
| -------- | ------------------------------------------------------------ |
| gencert  | 生成新的key(密钥)和签名证书                                  |
| -initca  | 初始化一个新ca                                               |
| -ca-key  | 指明ca的证书                                                 |
| -config  | 指明ca的私钥文件                                             |
| -profile | 指明请求证书的json文件                                       |
| -ca      | 与config中的profile对应，是指根据config中的profile段来生成证书的相关信息 |

### 4.4 分发证书

```bash
for ip in 172.16.0.81 172.16.0.82 172.16.0.83
do
  ssh root@${ip} "mkdir -pv /etc/etcd/ssl"
  scp ../ca/ca*.pem  root@${ip}:/etc/etcd/ssl
  scp ./etcd*.pem  root@${ip}:/etc/etcd/ssl
done
```

### 4.5 部署 ETCD

```bash
# 下载ETCD安装包
wget https://mirrors.huaweicloud.com/etcd/v3.3.24/etcd-v3.3.24-linux-amd64.tar.gz

# 解压
tar -xf etcd-v3.3.24-linux-amd64.tar.gz 

# 分发至其他节点
for i in 172.16.0.81 172.16.0.82 172.16.0.83
do
scp ./etcd-v3.3.24-linux-amd64/etcd* root@$i:/usr/local/bin/
done
```

### 4.6 注册 Etcd 服务

将etcd注册systemd服务主要是为了方便管理ETCD。

```bash
mkdir -pv /etc/kubernetes/conf/etcd

ETCD_NAME=`hostname`
INTERNAL_IP=`hostname -i | awk '{print $4}'`
INITIAL_CLUSTER=k8s-m-001=https://192.168.15.81:2380,k8s-m-002=https://192.168.15.82:2380,k8s-m-003=https://192.168.15.83:2380

cat << EOF | sudo tee /usr/lib/systemd/system/etcd.service
[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
ExecStart=/usr/local/bin/etcd \\
  --name ${ETCD_NAME} \\
  --cert-file=/etc/etcd/ssl/etcd.pem \\
  --key-file=/etc/etcd/ssl/etcd-key.pem \\
  --peer-cert-file=/etc/etcd/ssl/etcd.pem \\
  --peer-key-file=/etc/etcd/ssl/etcd-key.pem \\
  --trusted-ca-file=/etc/etcd/ssl/ca.pem \\
  --peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \\
  --peer-client-cert-auth \\
  --client-cert-auth \\
  --initial-advertise-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-peer-urls https://${INTERNAL_IP}:2380 \\
  --listen-client-urls https://${INTERNAL_IP}:2379,https://127.0.0.1:2379 \\
  --advertise-client-urls https://${INTERNAL_IP}:2379 \\
  --initial-cluster-token etcd-cluster \\
  --initial-cluster ${INITIAL_CLUSTER} \\
  --initial-cluster-state new \\
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF


systemctl daemon-reload
systemctl enable --now etcd


ETCDCTL_API=3 etcdctl \
--cacert=/etc/etcd/ssl/etcd.pem \
--cert=/etc/etcd/ssl/etcd.pem \
--key=/etc/etcd/ssl/etcd-key.pem \
--endpoints="https://192.168.15.81:2379,https://192.168.15.82:2379,https://192.168.15.83:2379" \
endpoint status --write-out='table'
```

### 4.7 配置项详解

| 配置选项                    | 选项说明                                   |
| --------------------------- | ------------------------------------------ |
| name                        | 节点名称                                   |
| data-dir                    | 指定节点的数据存储目录                     |
| listen-peer-urls            | 与集群其它成员之间的通信地址               |
| listen-client-urls          | 监听本地端口，对外提供服务的地址           |
| initial-advertise-peer-urls | 通告给集群其它节点，本地的对等URL地址      |
| advertise-client-urls       | 客户端URL，用于通告集群的其余部分信息      |
| initial-cluster             | 集群中的所有信息节点                       |
| initial-cluster-token       | 集群的token，整个集群中保持一致            |
| initial-cluster-state       | 初始化集群状态，默认为new                  |
| --cert-file                 | 客户端与服务器之间TLS证书文件的路径        |
| --key-file                  | 客户端与服务器之间TLS密钥文件的路径        |
| --peer-cert-file            | 对等服务器TLS证书文件的路径                |
| --peer-key-file             | 对等服务器TLS密钥文件的路径                |
| --trusted-ca-file           | 签名client证书的CA证书，用于验证client证书 |
| --peer-trusted-ca-file      | 签名对等服务器证书的CA证书。               |
| --trusted-ca-file           | 签名client证书的CA证书，用于验证client证书 |
| --peer-trusted-ca-file      | 签名对等服务器证书的CA证书。               |

### 4.8 启动ETCD

```bash
systemctl daemon-reload
systemctl enable --now etcd
systemctl status etcd

[root@k8s-m-001 etcd]# ETCDCTL_API=3 etcdctl --cacert=/etc/etcd/ssl/etcd.pem --cert=/etc/etcd/ssl/etcd.pem --key=/etc/etcd/ssl/etcd-key.pem --endpoints="https://192.168.15.81:2379,https://192.168.15.82:2379,https://192.168.15.83:2379" endpoint status --write-out='table'
+----------------------------+------------------+---------+---------+-----------+-----------+------------+
|          ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |
+----------------------------+------------------+---------+---------+-----------+-----------+------------+
| https://192.168.15.81:2379 | 4dcf01035d245f78 |  3.3.24 |   20 kB |     false |         5 |          9 |
| https://192.168.15.82:2379 | 2a92df6da315e029 |  3.3.24 |   20 kB |     false |         5 |          9 |
| https://192.168.15.83:2379 | 971c303d5d042151 |  3.3.24 |   20 kB |      true |         5 |          9 |
+----------------------------+------------------+---------+---------+-----------+-----------+------------+
```

## 五、生成 k8s 集群证书

Master节点是集群当中最为重要的一部分，组件众多，部署也最为复杂。

### 5.1 创建集群根证书

```bash
mkdir /opt/cert/k8s
cd /opt/cert/k8s
cat > ca-config.json << EOF
{
  "signing": {
    "default": {
      "expiry": "87600h"
    },
    "profiles": {
      "kubernetes": {
         "expiry": "87600h",
         "usages": [
            "signing",
            "key encipherment",
            "server auth",
            "client auth"
        ]
      }
    }
  }
}
EOF
```

### 5.2 创建集群根证书签名

```bash
cat > ca-csr.json << EOF
{
    "CN": "kubernetes",
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "ShangHai",
            "ST": "ShangHai"
        }
    ]
}
EOF
```

### 5.3 生成根证书

```bash
cfssl gencert -initca ca-csr.json | cfssljson -bare ca -
```

### 5.4 签发kube-apiserver证书

这个证书主要是其他组件跟 kube-apiserver 进行通信时使用。

```bash
cat > server-csr.json << EOF
{
    "CN": "kubernetes",
    "hosts": [
        "127.0.0.1",
        "192.168.15.81",
        "192.168.15.82",
        "192.168.15.83",
        "192.168.15.84",
        "192.168.15.85",
        "192.168.15.93",
        "10.96.0.1",
        "kubernetes",
        "kubernetes.default",
        "kubernetes.default.svc",
        "kubernetes.default.svc.cluster",
        "kubernetes.default.svc.cluster.local"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "ShangHai",
            "ST": "ShangHai"
        }
    ]
}
EOF
```

host：localhost地址 + master部署节点的ip地址 + etcd节点的部署地址 + 负载均衡指定的VIP(172.16.0.55) + service ip段的第一个合法地址(10.96.0.1) + k8s默认指定的一些地址。

#### 生成证书

```bash
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server
```

### 5.5 签发 kube-controller-manager 证书

```bash
cat > kube-controller-manager-csr.json << EOF
{
    "CN": "system:kube-controller-manager",
    "hosts": [
        "127.0.0.1",
        "192.168.15.81",
        "192.168.15.82",
        "192.168.15.83",
        "192.168.15.84",
        "192.168.15.85",
        "192.168.15.93"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "BeiJing",
            "ST": "BeiJing",
            "O": "system:kube-controller-manager",
            "OU": "System"
        }
    ]
}
EOF
```

#### 生成证书

```bash
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager
```

### 5.6 签发kube-scheduler证书

```bash
cat > kube-scheduler-csr.json << EOF
{
    "CN": "system:kube-scheduler",
    "hosts": [
        "127.0.0.1",
        "192.168.15.81",
        "192.168.15.82",
        "192.168.15.83",
        "192.168.15.84",
        "192.168.15.85",
        "192.168.15.93"
    ],
    "key": {
        "algo": "rsa",
        "size": 2048
    },
    "names": [
        {
            "C": "CN",
            "L": "BeiJing",
            "ST": "BeiJing",
            "O": "system:kube-scheduler",
            "OU": "System"
        }
    ]
}
EOF

```

#### 生成证书

```bash
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler
```

### 5.7 签发kube-proxy证书

```bash
cat > kube-proxy-csr.json << EOF
{
    "CN":"system:kube-proxy",
    "hosts":[],
    "key":{
        "algo":"rsa",
        "size":2048
    },
    "names":[
        {
            "C":"CN",
            "L":"BeiJing",
            "ST":"BeiJing",
            "O":"system:kube-proxy",
            "OU":"System"
        }
    ]
}
EOF
```

#### 生成证书

```bash
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy
```

### 5.8 签发管理员用户证书

```bash
cat > admin-csr.json << EOF
{
    "CN":"admin",
    "key":{
        "algo":"rsa",
        "size":2048
    },
    "names":[
        {
            "C":"CN",
            "L":"BeiJing",
            "ST":"BeiJing",
            "O":"system:masters",
            "OU":"System"
        }
    ]
}
EOF
```

#### 生成证书

```bash
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes admin-csr.json | cfssljson -bare admin
```



### 5.9 颁发证书

Master节点所需证书：ca、kube-apiservver、kube-controller-manager、kube-scheduler、用户证书、Etcd证书。

```bash
rm -rf /etc/kubernetes/ssl
mkdir -pv /etc/kubernetes/ssl

cp -p ./{ca*pem,server*pem,kube-controller-manager*pem,kube-scheduler*.pem,kube-proxy*pem,admin*.pem} /etc/kubernetes/ssl

for i in 172.16.0.82 172.16.0.83; do
  ssh root@$i "rm -rf /etc/kubernetes/ssl"
  ssh root@$i "mkdir -pv /etc/kubernetes/ssl"
  scp /etc/kubernetes/ssl/* root@$i:/etc/kubernetes/ssl
done
```

## 六、部署 master 节点组件

证书生成完毕之后，接下来就可以部署 Master 节点的各个组件啦。

### 6.1 下载二进制组件

```bash
wget https://dl.k8s.io/v1.24.2/kubernetes-server-linux-amd64.tar.gz
```

### 6.2 分发组件

```bash
for i in 172.16.0.81 172.16.0.82 172.16.0.83;  do   scp kube-apiserver kube-controller-manager kube-scheduler kube-proxy kubelet kubectl  root@$i:/usr/local/bin/; done
```

### 6.3 生成集群配置文件

在kubernetes中，我们需要创建一个配置文件，用来配置集群、用户、命名空间及身份认证等信息。

#### 6.3.1 创建 kube-controller-manager.kubeconfig 文件

```bash
export KUBE_APISERVER="https://192.168.15.93:8443"

# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-controller-manager.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials "kube-controller-manager" \
  --client-certificate=/etc/kubernetes/ssl/kube-controller-manager.pem \
  --client-key=/etc/kubernetes/ssl/kube-controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-controller-manager.kubeconfig

# 设置上下文参数（在上下文参数中将集群参数和用户参数关联起来）
kubectl config set-context default \
  --cluster=kubernetes \
  --user="kube-controller-manager" \
  --kubeconfig=kube-controller-manager.kubeconfig

# 配置默认上下文
kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig
```

- --certificate-authority：验证 kube-apiserver 证书的根证书。
- --client-certificate、--client-key：刚生成的kube-controller-manager证书和私钥，连接 kube-apiserver 时使用。
- --embed-certs=true：将ca.pem和kube-controller-manager 证书内容嵌入到生成的 kubectl.kubeconfig 文件中(不加时，写入的是证书文件路径)。

#### 6.3.2 创建 kube-scheduler.kubeconfig 文件

```bash
export KUBE_APISERVER="https://192.168.15.93:8443"

# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-scheduler.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials "kube-scheduler" \
  --client-certificate=/etc/kubernetes/ssl/kube-scheduler.pem \
  --client-key=/etc/kubernetes/ssl/kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig

# 设置上下文参数（在上下文参数中将集群参数和用户参数关联起来）
kubectl config set-context default \
  --cluster=kubernetes \
  --user="kube-scheduler" \
  --kubeconfig=kube-scheduler.kubeconfig

# 配置默认上下文
kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig
```

#### 6.3.3 创建kube-proxy.kubeconfig文件

```bash
export KUBE_APISERVER="https://192.168.15.93:8443"

# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kube-proxy.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials "kube-proxy" \
  --client-certificate=/etc/kubernetes/ssl/kube-proxy.pem \
  --client-key=/etc/kubernetes/ssl/kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig

# 设置上下文参数（在上下文参数中将集群参数和用户参数关联起来）
kubectl config set-context default \
  --cluster=kubernetes \
  --user="kube-proxy" \
  --kubeconfig=kube-proxy.kubeconfig

# 配置默认上下文
kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
```

#### 6.3.4 创建admin.kubeconfig文件

```bash
export KUBE_APISERVER="https://192.168.15.93:8443"

# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=admin.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials "admin" \
  --client-certificate=/etc/kubernetes/ssl/admin.pem \
  --client-key=/etc/kubernetes/ssl/admin-key.pem \
  --embed-certs=true \
  --kubeconfig=admin.kubeconfig

# 设置上下文参数（在上下文参数中将集群参数和用户参数关联起来）
kubectl config set-context default \
  --cluster=kubernetes \
  --user="admin" \
  --kubeconfig=admin.kubeconfig

# 配置默认上下文
kubectl config use-context default --kubeconfig=admin.kubeconfig
```

### 6.4 部署组件

#### 6.4.1 配置TLS bootstrapping

TLS bootstrapping 是用来简化管理员配置kubelet 与 apiserver 双向加密通信的配置步骤的一种机制。当集群开启了 TLS 认证后，每个节点的 kubelet 组件都要使用由 apiserver 使用的 CA 签发的有效证书才能与 apiserver 通讯，此时如果有很多个节点都需要单独签署证书那将变得非常繁琐且极易出错，导致集群不稳。

TLS bootstrapping 功能就是让 node节点上的kubelet组件先使用一个预定的低权限用户连接到 apiserver，然后向 apiserver 申请证书，由 apiserver 动态签署颁发到Node节点，实现证书签署自动化。

##### 生成TLS bootstrapping所需token

```bash
# 必须要用自己机器创建的Token
TLS_BOOTSTRAPPING_TOKEN=`head -c 16 /dev/urandom | od -An -t x | tr -d ' '`

cat > token.csv << EOF
${TLS_BOOTSTRAPPING_TOKEN},kubelet-bootstrap,10001,"system:kubelet-bootstrap"
EOF
```

##### 创建TLS Bootstrapping集群配置文件

在kubernetes中，我们需要创建一个配置文件，用来配置集群、用户、命名空间及身份认证等信息。

```bash
export KUBE_APISERVER="https://192.168.15.93:8443"

# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=/etc/kubernetes/ssl/ca.pem \
  --embed-certs=true \
  --server=${KUBE_APISERVER} \
  --kubeconfig=kubelet-bootstrap.kubeconfig

# 设置客户端认证参数,此处token必须用上叙token.csv中的token
kubectl config set-credentials "kubelet-bootstrap" \
  --token=e061072e959b30257ff4794356b9f85d \
  --kubeconfig=kubelet-bootstrap.kubeconfig

# 设置上下文参数（在上下文参数中将集群参数和用户参数关联起来）
kubectl config set-context default \
  --cluster=kubernetes \
  --user="kubelet-bootstrap" \
  --kubeconfig=kubelet-bootstrap.kubeconfig

# 配置默认上下文
kubectl config use-context default --kubeconfig=kubelet-bootstrap.kubeconfig
```

##### 分发 TLS bootstrap 证书

```bash
for i in 172.16.0.81 172.16.0.82 172.16.0.83;
do
  ssh root@$i "mkdir -p  /etc/kubernetes/cfg";
  scp token.csv kube-scheduler.kubeconfig kube-controller-manager.kubeconfig admin.kubeconfig kube-proxy.kubeconfig kubelet-bootstrap.kubeconfig root@$i:/etc/kubernetes/cfg;
done
```

##### 授权TLS Bootrapping用户请求

```bash
kubectl create clusterrolebinding kubelet-bootstrap \
--clusterrole=system:node-bootstrapper \
--user=kubelet-bootstrap
```

#### 6.4.2 部署api-server

创建kube-apiserver服务配置文件(三个节点都要执行，不能复制，注意api server IP)。

```bash
export KUBE_APISERVER_IP="192.168.15.81"

cat > /etc/kubernetes/cfg/kube-apiserver.conf << EOF
KUBE_APISERVER_OPTS="--logtostderr=false \\
--v=2  \\
--logtostderr=true  \\
--allow-privileged=true  \\
--bind-address=0.0.0.0  \\
--secure-port=6443  \\
--advertise-address=${KUBE_APISERVER_IP} \\
--service-cluster-ip-range=10.96.0.0/16,2001:db8:42:1::/112  \\
--feature-gates=IPv6DualStack=true \\
--service-node-port-range=30000-32767  \\
--etcd-servers=https://192.168.15.81:2379,https://192.168.15.82:2379,https://192.168.15.83:2379 \\
--etcd-cafile=/etc/etcd/ssl/ca.pem \\
--etcd-certfile=/etc/etcd/ssl/etcd.pem \\
--etcd-keyfile=/etc/etcd/ssl/etcd-key.pem \\
--client-ca-file=/etc/kubernetes/ssl/ca.pem \\
--enable-bootstrap-token-auth=true \\
--token-auth-file=/etc/kubernetes/cfg/token.csv \\
--kubelet-client-certificate=/etc/kubernetes/ssl/server.pem \\
--kubelet-client-key=/etc/kubernetes/ssl/server-key.pem \\
--tls-cert-file=/etc/kubernetes/ssl/server.pem  \\
--tls-private-key-file=/etc/kubernetes/ssl/server-key.pem \\
--service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\
--service-account-issuer=https://kubernetes.default.svc.cluster.local \\
--service-account-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\
--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname  \\
--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,ResourceQuota  \\
--authorization-mode=Node,RBAC  \\
--enable-bootstrap-token-auth=true  \\
--requestheader-allowed-names=aggregator  \\
--requestheader-group-headers=X-Remote-Group  \\
--requestheader-extra-headers-prefix=X-Remote-Extra-  \\
--requestheader-username-headers=X-Remote-User \\
--enable-aggregator-routing=true \\
--token-auth-file=/etc/kubernetes/cfg/token.csv"
EOF
```

**参数详解**

| 配置选项                                 | 选项说明                                                     |
| ---------------------------------------- | ------------------------------------------------------------ |
| --logtostderr=false                      | 输出日志到文件中，不输出到标准错误控制台                     |
| --v=2                                    | 指定输出日志的级别                                           |
| --advertise-address                      | 向集群成员通知 apiserver 消息的 IP 地址                      |
| --etcd-servers                           | 连接的 etcd 服务器列表                                       |
| --etcd-cafile                            | 用于etcd 通信的 SSL CA 文件                                  |
| --etcd-certfile                          | 用于 etcd 通信的的 SSL 证书文件                              |
| --etcd-keyfile                           | 用于 etcd 通信的 SSL 密钥文件                                |
| --service-cluster-ip-range               | Service网络地址分配                                          |
| --bind-address                           | 监听 --seure-port 的 IP 地址，如果为空，则将使用所有接口（0.0.0.0） |
| --secure-port=6443                       | 用于监听具有认证授权功能的 HTTPS 协议的端口，默认值是6443    |
| --allow-privileged                       | 是否启用授权功能                                             |
| --service-node-port-range                | Service使用的端口范围                                        |
| --default-not-ready-toleration-seconds   | 表示 notReady状态的容忍度秒数                                |
| --default-unreachable-toleration-seconds | 表示 unreachable状态的容忍度秒数：                           |
| --max-mutating-requests-inflight=2000    | 在给定时间内进行中可变请求的最大数量，0 值表示没有限制（默认值 200） |
| --default-watch-cache-size=200           | 默认监视缓存大小，0 表示对于没有设置默认监视大小的资源，将禁用监视缓存 |
| --delete-collection-workers=2            | 用于 DeleteCollection 调用的工作者数量，这被用于加速 namespace 的清理( 默认值 1) |
| --enable-admission-plugins               | 资源限制的相关配置                                           |
| --authorization-mode                     | 在安全端口上进行权限验证的插件的顺序列表，以逗号分隔的列表。 |

##### **注册 kube-apiserver 服务**

```bash
cat > /usr/lib/systemd/system/kube-apiserver.service << EOF
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
EnvironmentFile=/etc/kubernetes/cfg/kube-apiserver.conf
ExecStart=/usr/local/bin/kube-apiserver \$KUBE_APISERVER_OPTS
Restart=on-failure
RestartSec=10
Type=notify
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

mkdir -p /var/log/kubernetes/
systemctl daemon-reload
systemctl enable --now kube-apiserver
systemctl status kube-apiserver
```

#### 6.4.3 高可用部署 kube-apiserver

负载均衡器有很多种，只要能实现api-server高可用都行，这里我们采用官方推荐的haproxy + keepalived。

##### 1. 安装高可用软件

```bash
yum install -y keepalived haproxy
```

##### 2. 配置haproxy服务

```bash
cat > /etc/haproxy/haproxy.cfg <<EOF
global
  maxconn  2000
  ulimit-n  16384
  log  127.0.0.1 local0 err
  stats timeout 30s

defaults
  log global
  mode  http
  option  httplog
  timeout connect 5000
  timeout client  50000
  timeout server  50000
  timeout http-request 15s
  timeout http-keep-alive 15s

frontend monitor-in
  bind *:33305
  mode http
  option httplog
  monitor-uri /monitor

listen stats
  bind    *:8006
  mode    http
  stats   enable
  stats   hide-version
  stats   uri       /stats
  stats   refresh   30s
  stats   realm     Haproxy\ Statistics
  stats   auth      admin:admin

frontend k8s-master
  bind 0.0.0.0:8443
  bind 127.0.0.1:8443
  mode tcp
  option tcplog
  tcp-request inspect-delay 5s
  default_backend k8s-master

backend k8s-master
  mode tcp
  option tcplog
  option tcp-check
  balance roundrobin
  default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100
  server k8s-m-001    192.168.15.81:6443  check inter 2000 fall 2 rise 2 weight 100
  server k8s-m-002    192.168.15.82:6443  check inter 2000 fall 2 rise 2 weight 100
  server k8s-m-003    192.168.15.83:6443  check inter 2000 fall 2 rise 2 weight 100
EOF
```

##### 3. 配置keepalived服务

```bash
mv /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf_bak

cd /etc/keepalived

cat > /etc/keepalived/keepalived.conf <<EOF
! Configuration File for keepalived
global_defs {
    router_id LVS_DEVEL
}
vrrp_instance VI_1 {
    state MASTER
    interface eth0
    mcast_src_ip 192.168.15.81
    virtual_router_id 51
    priority 100
    advert_int 2
    authentication {
        auth_type PASS
        auth_pass K8SHA_KA_AUTH
    }
    virtual_ipaddress {
        192.168.15.93
    }
}
EOF
```

#### 6.4.4 部署kube-controller-manager服务

Controller Manager作为集群内部的管理控制中心，负责集群内的Node、Pod副本、服务端点（Endpoint）、命名空间（Namespace）、服务账号（ServiceAccount）、资源定额（ResourceQuota）的管理，当某个Node意外宕机时，Controller Manager会及时发现并执行自动化修复流程，确保集群始终处于预期的工作状态。如果多个控制器管理器同时生效，则会有一致性问题，所以kube-controller-manager的高可用，只能是主备模式，而kubernetes集群是采用租赁锁实现leader选举，需要在启动参数中加入 --leader-elect=true。

##### 1. 创建kube-controller-manager配置文件

```bash
cat > /etc/kubernetes/cfg/kube-controller-manager.conf << EOF
KUBE_CONTROLLER_MANAGER_OPTS="--logtostderr=false \\
--v=2 \\
--log-dir=/var/log/kubernetes \\
--leader-elect=true \\
--cluster-name=kubernetes \\
--bind-address=127.0.0.1 \\
--allocate-node-cidrs=true \\
--cluster-cidr=10.244.0.0/16,2001:db8:42:0::/56 \\
--service-cluster-ip-range=10.96.0.0/16,2001:db8:42:1::/112  \\
--cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\
--cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem  \\
--root-ca-file=/etc/kubernetes/ssl/ca.pem \\
--service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\
--kubeconfig=/etc/kubernetes/cfg/kube-controller-manager.kubeconfig \\
--tls-cert-file=/etc/kubernetes/ssl/kube-controller-manager.pem \\
--tls-private-key-file=/etc/kubernetes/ssl/kube-controller-manager-key.pem \\
--experimental-cluster-signing-duration=87600h0m0s \\
--controllers=*,bootstrapsigner,tokencleaner \\
--use-service-account-credentials=true \\
--node-monitor-grace-period=10s \\
--node-monitor-period=5s \\
--pod-eviction-timeout=2m0s \\
--node-cidr-mask-size-ipv4=24 \\
--node-cidr-mask-size-ipv6=64 \\
--feature-gates=IPv6DualStack=true \\
--requestheader-client-ca-file=/etc/kubernetes/ssl/ca.pem"
EOF
```

##### 2. 配置文件详解

| 配置选项                                | 选项意义                                                     |
| --------------------------------------- | ------------------------------------------------------------ |
| --leader-elect                          | 高可用时启用选举功能。                                       |
| --master                                | 通过本地非安全本地端口8080连接apiserver                      |
| --bind-address                          | 监控地址                                                     |
| --allocate-node-cidrs                   | 是否应在node节点上分配和设置Pod的CIDR                        |
| --cluster-cidr                          | Controller Manager在启动时如果设置了--cluster-cidr参数，防止不同的节点的CIDR地址发生冲突 |
| --service-cluster-ip-range              | 集群Services 的CIDR范围                                      |
| --cluster-signing-cert-file             | 指定用于集群签发的所有集群范围内证书文件（根证书文件）       |
| --cluster-signing-key-file              | 指定集群签发证书的key                                        |
| --root-ca-file                          | 如果设置，该根证书权限将包含service acount的toker secret，这必须是一个有效的PEM编码CA 包 |
| --service-account-private-key-file      | 包含用于签署service account token的PEM编码RSA或者ECDSA私钥的文件名 |
| --experimental-cluster-signing-duration | 证书签发时间                                                 |

##### 3. 注册kube-controller-manager服务

```bash
cat > /usr/lib/systemd/system/kube-controller-manager.service << EOF
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
EnvironmentFile=/etc/kubernetes/cfg/kube-controller-manager.conf
ExecStart=/usr/local/bin/kube-controller-manager \$KUBE_CONTROLLER_MANAGER_OPTS
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable --now kube-controller-manager
systemctl status kube-controller-manager
```

#### 6.4.5 部署kube-scheduler服务

kube-scheduler是 Kubernetes 集群的默认调度器，并且是集群 控制面 的一部分。对每一个新创建的 Pod 或者是未被调度的 Pod，kube-scheduler 会过滤所有的node，然后选择一个最优的 Node 去运行这个 Pod。kube-scheduler 调度器是一个策略丰富、拓扑感知、工作负载特定的功能，调度器显著影响可用性、性能和容量。调度器需要考虑个人和集体的资源要求、服务质量要求、硬件/软件/政策约束、亲和力和反亲和力规范、数据局部性、负载间干扰、完成期限等。工作负载特定的要求必要时将通过 API 暴露。

##### 1. 创建kube-scheduler配置文件

```bash
cat > /etc/kubernetes/cfg/kube-scheduler.conf << EOF
KUBE_SCHEDULER_OPTS="--logtostderr=false \\
--v=2 \\
--log-dir=/var/log/kubernetes \\
--kubeconfig=/etc/kubernetes/cfg/kube-scheduler.kubeconfig \\
--leader-elect=true \\
--master=http://127.0.0.1:8080 \\
--bind-address=127.0.0.1"
EOF
```

##### 2.创建启动脚本

```bash
cat > /usr/lib/systemd/system/kube-scheduler.service << EOF
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes
After=network.target

[Service]
EnvironmentFile=/etc/kubernetes/cfg/kube-scheduler.conf
ExecStart=/usr/local/bin/kube-scheduler \$KUBE_SCHEDULER_OPTS
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable --now kube-scheduler
systemctl status kube-scheduler
```

##### 3.查看集群状态

```bash
[root@k8s-m-001 keepalived]# export KUBECONFIG=/etc/kubernetes/cfg/admin.kubeconfig
[root@k8s-m-001 keepalived]# kubectl get cs
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE             ERROR
scheduler            Healthy   ok                  
controller-manager   Healthy   ok                  
etcd-1               Healthy   {"health":"true"}   
etcd-2               Healthy   {"health":"true"}   
etcd-0               Healthy   {"health":"true"} 
```

#### 6.4.6 部署kubelet服务

kubelet 是负责创建和监控容器。

##### 1. 创建kubelet配置

```bash
KUBE_HOSTNAME=`hostname`

cat > /etc/kubernetes/cfg/kubelet.conf << EOF
KUBELET_OPTS="--logtostderr=false \\
--v=2 \\
--log-dir=/var/log/kubernetes \\
--hostname-override=${KUBE_HOSTNAME} \\
--container-runtime=remote \\
--kubeconfig=/etc/kubernetes/cfg/kubelet.kubeconfig \\
--bootstrap-kubeconfig=/etc/kubernetes/cfg/kubelet-bootstrap.kubeconfig \\
--config=/etc/kubernetes/cfg/kubelet-config.yml \\
--cert-dir=/etc/kubernetes/ssl \\
--runtime-request-timeout=15m \\
--container-runtime-endpoint=unix:///run/containerd/containerd.sock  \\
--cgroup-driver=systemd \\
--node-labels=node.kubernetes.io/node= \\
--feature-gates=IPv6DualStack=true"
EOF
```

##### 2. 创建kubelet-config配置文件

```bash
cat > /etc/kubernetes/cfg/kubelet-config.yml << EOF
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
address: 0.0.0.0
port: 10250
readOnlyPort: 10255
cgroupDriver: cgroupfs
clusterDNS:
- 10.0.0.2
clusterDomain: cluster.local 
failSwapOn: false
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 2m0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/ssl/ca.pem 
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 5m0s
    cacheUnauthorizedTTL: 30s
evictionHard:
  imagefs.available: 15%
  memory.available: 100Mi
  nodefs.available: 10%
  nodefs.inodesFree: 5%
maxOpenFiles: 1000000
maxPods: 110
EOF
```

| 配置选项                       | 选项意义                                                     |
| ------------------------------ | ------------------------------------------------------------ |
| --hostname-override            | 用来配置该节点在集群中显示的主机名，kubelet设置了-–hostname-override参数后，kube-proxy也需要设置，否则会出现找不到Node的情况 |
| --container-runtime            | 指定容器运行时引擎                                           |
| --kubeconfig                   | kubelet作为客户端使用的kubeconfig认证文件，此文件是由kube-controller-mananger自动生成的 |
| --bootstrap-kubeconfig         | 指定令牌认证文件                                             |
| --config                       | 指定kubelet配置文件                                          |
| --cert-dir                     | 设置kube-controller-manager生成证书和私钥的目录              |
| --image-pull-progress-deadline | 镜像拉取进度最大时间，如果在这段时间拉取镜像没有任何进展，将取消拉取，默认：1m0s |
| --pod-infra-container-image    | 每个pod中的network/ipc 名称空间容器将使用的镜像              |

##### 3. 创建kubelet启动脚本

```bash
kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap

cat > /usr/lib/systemd/system/kubelet.service << EOF
[Unit]
Description=Kubernetes Kubelet
After=docker.service

[Service]
EnvironmentFile=/etc/kubernetes/cfg/kubelet.conf
ExecStart=/usr/local/bin/kubelet \$KUBELET_OPTS
Restart=on-failure
RestartSec=10
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable --now kubelet
systemctl status kubelet.service
```

#### 6.4.7 配置kube-proxy服务

kube-proxy是Kubernetes的核心组件，部署在每个Node节点上，它是实现Kubernetes Service的通信与负载均衡机制的重要组件; kube-proxy负责为Pod创建代理服务，从apiserver获取所有server信息，并根据server信息创建代理服务，实现server到Pod的请求路由和转发，从而实现K8s层级的虚拟转发网络。

##### 1. 创建kube-proxy配置文件

```bash
cat > /etc/kubernetes/cfg/kube-proxy.conf << EOF
KUBE_PROXY_OPTS="--logtostderr=false \\
--v=2 \\
--log-dir=/var/log/kubernetes \\
--config=/etc/kubernetes/cfg/kube-proxy-config.yml"
EOF
```

##### 2. 创建kube-proxy-config配置文件

```bash
KUBE_HOSTNAME=`hostname`

cat > /etc/kubernetes/cfg/kube-proxy-config.yml << EOF
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
bindAddress: 0.0.0.0
healthzBindAddress: 0.0.0.0:10256
metricsBindAddress: 0.0.0.0:10249
clientConnection:
  burst: 200
  kubeconfig: /etc/kubernetes/cfg/kube-proxy.kubeconfig
  qps: 100
hostnameOverride: ${KUBE_HOSTNAME}
clusterCIDR: 10.244.0.0/16
enableProfiling: true
mode: "ipvs"
ipvs:
  scheduler: "rr"
iptables:
  masqueradeAll: true
EOF
```

| 选项配置           | 选项意义                                                     |
| ------------------ | ------------------------------------------------------------ |
| clientConnection   | 与kube-apiserver交互时的参数设置                             |
| burst: 200         | 临时允许该事件记录值超过qps设定值                            |
| kubeconfig         | kube-proxy 客户端连接 kube-apiserver 的 kubeconfig 文件路径设置 |
| qps: 100           | 与kube-apiserver交互时的QPS，默认值5                         |
| bindAddress        | kube-proxy监听地址                                           |
| healthzBindAddress | 用于检查服务的IP地址和端口                                   |
| metricsBindAddress | metrics服务的ip地址和端口。默认：127.0.0.1:10249             |
| clusterCIDR        | kube-proxy 根据 --cluster-cidr 判断集群内部和外部流量，指定 --cluster-cidr 或 --masquerade-all 选项后 kube-proxy 才会对访问 Service IP 的请求做 SNAT |
| hostnameOverride   | 参数值必须与 kubelet 的值一致，否则 kube-proxy 启动后会找不到该 Node，从而不会创建任何 ipvs 规则； |
| masqueradeAll      | 如果使用纯iptables代理，SNAT所有通过服务集群ip发送的通信     |
| mode               | 使用ipvs模式                                                 |
| scheduler          | 当proxy为ipvs模式时，ipvs调度类型                            |

##### 3. 创建kube-proxy启动脚本

```bash
cat > /usr/lib/systemd/system/kube-proxy.service << EOF

[Unit]
Description=Kubernetes Proxy
After=network.target

[Service]
EnvironmentFile=/etc/kubernetes/cfg/kube-proxy.conf
ExecStart=/usr/local/bin/kube-proxy \$KUBE_PROXY_OPTS
Restart=on-failure
RestartSec=10
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
EOF

systemctl daemon-reload
systemctl enable --now kube-proxy
systemctl status kube-proxy
```

#### 6.4.8 查看kubelet加入集群请求并批准加入

```bash
kubectl get csr

kubectl certificate approve `kubectl get csr | grep "Pending" | awk '{print $1}'`

[root@k8s-m-001 ~]# kubectl get nodes
NAME        STATUS     ROLES    AGE   VERSION
k8s-m-001   Ready      <none>   18s   v1.24.2
k8s-m-002   NotReady   <none>   17s   v1.24.2
k8s-m-003   NotReady   <none>   17s   v1.24.2
```

#### 6.4.9 设置集群角色

```bash
kubectl label nodes k8s-m-001 node-role.kubernetes.io/master=k8s-m-001
kubectl label nodes k8s-m-002 node-role.kubernetes.io/master=k8s-m-002
kubectl label nodes k8s-m-003 node-role.kubernetes.io/master=k8s-m-003

[root@k8s-m-001 ~]# kubectl get nodes
NAME        STATUS     ROLES    AGE   VERSION
k8s-m-001   Ready      master   70s   v1.24.2
k8s-m-002   NotReady   master   69s   v1.24.2
k8s-m-003   NotReady   master   69s   v1.24.2
```

#### 6.4.10 命令提示

```bash
yum install -y bash-completion
source /usr/share/bash-completion/bash_completion
source <(kubectl completion bash)
echo "source <(kubectl completion bash)" >> ~/.bashrc
```



## 七、部署 Calico 网络插件

网络组件有很多种，只需要部署其中一个即可，推荐Calico。

Calico是一个纯三层的数据中心网络方案，Calico支持广泛的平台，包括Kubernetes、OpenStack等。

Calico 在每一个计算节点利用 Linux Kernel 实现了一个高效的虚拟路由器（ vRouter） 来负责数据转发，而每个 vRouter 通过 BGP 协议负责把自己上运行的 workload 的路由信息向整个 Calico 网络内传播。

此外，Calico 项目还实现了 Kubernetes 网络策略，提供ACL功能。

```bash
$ curl https://projectcalico.docs.tigera.io/manifests/calico-typha.yaml -o calico.yaml
$ vim calico.yaml
    "ipam": {
        "type": "calico-ipam",
        "assign_ipv4": "true",
        "assign_ipv6": "true"
    },
    - name: IP
      value: "autodetect"

    - name: IP6
      value: "autodetect"

    - name: CALICO_IPV4POOL_CIDR
      value: "10.244.0.0/16"

    - name: CALICO_IPV6POOL_CIDR
      value: "2001:db8:42:0::/56"

    - name: FELIX_IPV6SUPPORT
      value: "true"

$ kubectl apply -f calico.yaml 
```

## 八、安装CoreDNS

CoreDNS 用于集群中 Pod 解析 Service 的名字，Kubernetes 基于 CoreDNS 用于服务发现功能。

### 8.1 下载配置文件

```bash
git clone https://github.com/coredns/deployment.git
cd deployment/kubernetes/
```

### 8.2 部署 CoreDNS

```bash
./deploy.sh -i 10.96.0.2 -s | kubectl apply -f -
```

### 8.3 查看结果

```bash
```

