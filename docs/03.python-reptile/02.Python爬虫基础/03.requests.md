---
title: requests
date: 2022-07-16 14:45:42
permalink: /pages/1ab82e/
categories:
  - python-reptile
  - Python爬虫基础
tags:
  - 
author: 
  name: 小阳爱技术
  link: https://blog.abck8s.com
---
# Python逆向爬虫之requests

requests 模块是 python 基于 urllib，采用 Apache2 Licensed 开源协议的 HTTP 库。它比 urllib 更加方便，可以节约我们大量的工作，完全满足 HTTP 测试需求。

## 一、安装

```bash
pip install requests
```

## 二、基本语法

| 方法             | 说明                                                      |
| ---------------- | --------------------------------------------------------- |
| requsts.requst() | 构造一个请求，最基本的方法，是下面方法的支撑              |
| requsts.get()    | 获取网页，对应HTTP中的GET方法                             |
| requsts.post()   | 向网页提交信息，对应HTTP中的POST方法                      |
| requsts.head()   | 获取html网页的头信息，对应HTTP中的HEAD方法                |
| requsts.put()    | 向html提交put方法，对应HTTP中的PUT方法                    |
| requsts.patch()  | 向html网页提交局部请求修改的的请求，对应HTTP中的PATCH方法 |
| requsts.delete() | 向html提交删除请求，对应HTTP中的DELETE方法                |

### 2.1 requsts.get()

| 参数    |  类型  | 作用                                                         |
| ------- | :----: | ------------------------------------------------------------ |
| params  |  字典  | url为基准的url地址，不包含查询参数；该方法会自动对params字典编码,然后和url拼接 |
| url     | 字符串 | requests 发起请求的地址                                      |
| headers |  字典  | 请求头，发送请求的过程中请求的附加内容携带着一些必要的参数   |
| cookies |  字典  | 携带登录状态                                                 |
| proxies |  字典  | 用来设置代理 ip 服务器                                       |
| timeout |  整型  | 用于设定超时时间， 单位为秒                                  |

#### 2.1.1 response.text和response.content的区别

1. response.content ：这个是直接从网络上抓取的数据，没有经过任何的编码，所以是一个bytes类型，其实在硬盘上和网络上传输的字符串都是bytes类型
2. response.text：这个是str的数据类型，是requests库将response.content进行解码的字符串，解码需要指定一个编码方式，requests会根据自己的猜测来判断编码的方式，所以有时候可能会猜测错误，就会导致解码产生乱码，这时候就应该进行手动解码，比如使用response.content.decode('utf-8')

### 2.2 requsts.post()

```python
import requests

url = 'url'
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'
}
data = {
    'redirect': 'url',
    'username': 'xxx',
    'password': 'xxxx'
}
resp = requests.post(url,headers=headers,data=data)
print(resp.text)
```

### 2.3 cookie

基本使用：模拟登陆

```python
import requests
url = 'url'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36',
    'cookie':''
}
resp = requests.get(url,headers=headers)
print(resp.text)
```

### 2.4 session：共享cookie

```python
post_url = ''

post_data = {
    'username':'',
    'password':'.'
}
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.86 Safari/537.36'
}

# 登录
session = requests.session()
session.post(post_url,headers=headers,data=post_data)


#访问个人网页
url = 'url'

resp = session.get(url)
print(resp.text)
```

### 2.5 处理不信任的SSL证书

对于那些已经被信任的SSL证书的网站，比如https://www.baidu.com/，那么使用requests直接就可以正常的返回响应。示例代码如下：

```python
resp = requests.get('',verify=False)
print(resp.content.decode('utf-8'))
```

### 2.6 超时设置

在本机网络状况不好或者服务器网络响应太慢甚至无响应时，我们可能会等待特别久的时间才可能收到响应，甚至到最后收不到响应而报错。为了应对这种情况，应设置一个超时时间，这个时间是计算机发出请求到服务器返回响应的时间，如果请求超过了这个超时时间还没有得到响应，就抛出错误。这就需要使用timeout参数实现，单位为秒。

#### 2.6.1 指定请求总的超时时间

```python
import requests
#向淘宝发出请求，如果1秒内没有得到响应，则抛出错误
r = requests.get('https://www.taobao.com',timeout=1)
print(r.status_code)
```

#### 2.6.2 分别指定超时时间

实际上，请求分为两个阶段：连接（connect）和读取（read）。如果给timeout参数指定一个整数值，则超时时 间是这两个阶段的总和；如果要分别指定，就可以传入一个元组，连接超时时间和读取超时时间

```python
import requests
#向淘宝发出请求，如果连接阶段5秒内没有得到响应或读取阶段30秒内没有得到响应，则抛出错误
r = requests.get('https://www.taobao.com',timeout=(5,30))
print(r.status_code)
```

### 2.7 身份验证

#### 2.7.1 HTTPBasicAuth

此时可以使用requests自带的身份验证功能，通过HTTPBasicAuth类实现。

```python
import requests
from requests.auth import HTTPBasicAuth
r = requests.get('http://localhost:8080/manager/html',auth=HTTPBasicAuth('admin','123456'))
print(r.status_code)
```

