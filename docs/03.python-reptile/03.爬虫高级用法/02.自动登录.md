# Python爬虫之自动登录

## 一、会话技术

通常，我们会有这样的一种场景。

我们用一个聊天工具跟一个朋友聊天，假如现在需要换一部手机继续跟这个朋友聊天的话，很有可能我们上面所有的聊天记录全部没有啦？

这个时候，我们就需要一个用来记录前后聊天记录的这样的一个功能，这个功能的我们称之为：会话技术。

在会话技术中，有两个关键的部分，分别是 cookie 和 session。

### 1.1 cookie

将会话中产生的数据保存在客户端。

浏览器向服务器发送请求，其中只包含需要保存的数据，服务器获取数据，通过set-cookie响应头将数据响应给浏览器，让其保存。
浏览器再次访问服务器时，会在请求中通过cookie请求头携带上次保存的数据，服务器可以根据cookie请求头获取数据。

### 1.2 session

将会话中产生的数据保存在服务器端。

浏览器向服务器发送请求，服务器接收请求参数，然后在服务器中检查是否有为当前浏览器服务的session，如果有则直接拿来使用，如果没有则创建一个session，并将数据保存在session中。
当浏览器再次访问服务器时，服务器可以找到为当前浏览器服务的session，从中取出数据。
每个session对应一个id，此id通过cookie（临时）保存在浏览器中，访问服务器时携带cookie，服务器根据这个id判断是那个session。

## 二、使用 cookie 自动登录豆丁

```python
# -*- coding: utf-8 -*-
import requests
from bs4 import BeautifulSoup

def main():
    url_basic = 'https://www.docin.com/app/loginAjax/userLoginAjax.do?t=1658136370415'
    ua_headers = {
        "User-Agent": 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',
        "Referer": "https://www.docin.com/"
    }

    data = {
        'username': '15517127859',
        'password': '15517127859',
        'showcode4login': 0,
        'loginfalsecode': ''
    }

    session = requests.session()

    # 登录
    data = session.post(url=url_basic, headers=ua_headers, data=data)

    # 登录成功之后，开始爬取数据
    if data.text == '1':
        print("登录成功！")
    else:
        print("登录失败！")

if __name__ == '__main__':
    main()
```

## 三、防盗链自动爬取 B站 视频

```python
# -*- coding: utf-8 -*-
from bs4 import BeautifulSoup
import requests
import json
import subprocess

baseUrl = "https://www.bilibili.com/video/BV1re4y1X76e?spm_id_from=333.851.b_7265636f6d6d656e64.1"

html = requests.get(url=baseUrl)

htmlText = BeautifulSoup(html.text, 'lxml')

htmlValue = htmlText.find_all('script')[3].text

value = json.loads(htmlValue.replace("window.__playinfo__=", ""))

# 下载视频
header = {
    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36",
    "referer": "https://www.bilibili.com/bangumi/play/ep607472?spm_id_from=333.1007.partition_recommend.content.click"
}

for i in value['data']['dash']['video']:
    video = requests.get(url=i['base_url'], headers=header)
    with open('./1.mp4', 'wb') as values:
        values.write(video.content)

for i in value['data']['dash']['audio']:
    video = requests.get(url=i['base_url'], headers=header)
    with open('./1.mp3', 'wb') as values:
        values.write(video.content)

merge_cmd = "ffmpeg -i 1.mp3 -i 1.mp4 outputname.mp4"
subprocess.call(merge_cmd, shell=True)
```

## 四、访问代理

```python
# -*- coding: utf-8 -*-
import requests

proxy = {
    "http": "http://121.40.169.87:22",
    "https": "https://121.40.169.87:22"
}

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.127 Safari/537.36'
}

res = requests.get('https://www.baidu.com/', proxies=proxy, headers=headers, verify=False)
res.encoding = 'utf-8'

print(res.text)
```

